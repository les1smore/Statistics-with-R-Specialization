---
title: "Peer Assessment I"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---


First, let us load the data and necessary packages:

```{r load, message = FALSE}
load("ames_train.Rdata")
library(MASS)
library(dplyr)
library(ggplot2)
```

#
Make a labeled histogram (with 30 bins) of the ages of the houses in the data set, and describe the distribution.


```{r Q1}

ames_train$Age <- 
  sapply(ames_train$Year.Built, function(x) 2020 - x)

ggplot(data = ames_train, aes(x = Age, y = ..density..)) +
  geom_histogram(bins = 30) +
  labs(title = "Ages of houses within years", x = 'Age', y = "Frequency Count")

```

* * *

The histogram above shows the distribution is right-skewed with multimodality. And there's no data at age 0 since no house age can be less than 0 year. Meanwhile, with the increase of houses' age, more of them would be replaced or redecorated with new ones, resulting less old houses in this dataset. 

* * *


#
The mantra in real estate is "Location, Location, Location!" Make a graphical display that relates a home price to its neighborhood in Ames, Iowa. Which summary statistics are most appropriate to use for determining the most expensive, least expensive, and most heterogeneous (having the most variation in housing price) neighborhoods? Report which neighborhoods these are based on the summary statistics of your choice. Report the value of your chosen summary statistics for these neighborhoods.


```{r Q2}
ggplot(ames_train,aes(x = Neighborhood, y = price/10000)) +
  geom_boxplot() +
  labs(title = "Housing Price in Each Neighborhood", x = "Neighborhood", y = "price/$10000") +
  theme(axis.text.x = element_text(angle = 90)) 

```

* * *


Standard deviation would be a helpful statistic to determine the spread variation of houses price among neighborhoods. To identify the most and least expensive neighborhood, we would take median as a good factor to measure. 


* * *

```{r}
ames_train %>%
  group_by(Neighborhood) %>%
  summarise(sd = sd(price, na.rm = TRUE), median = median(price, na.rm = TRUE)) %>%
  arrange(desc(median)) 
```

* * *

The most expensive neighborhood is StoneBr, with median price $340691.5

The least expensive neighborhood is Meadow V, with median cost $85750

The most heterogeneous neighborhood is StoneBr, with standard deviation 123459.10

* * *



# 

Which variable has the largest number of missing values? Explain why it makes sense that there are so many missing values for this variable.

```{r Q3}
# type your code for Question 3 here, and Knit
colSums(is.na(ames_train))
```


* * *


According to the result, "Pool.QC" has the largest number of missing values, which is 997. Pool.QC is a variable demonstrating swimming pool quality.According to 0 swimming pool in Pool.Area field, it makes sense that this area has so many missing values for the Pool.QC variable.


* * *

#

We want to predict the natural log of the home prices. Candidate explanatory variables are lot size in square feet (Lot.Area), slope of property (Land.Slope), original construction date (Year.Built), remodel date (Year.Remod.Add), and the number of bedrooms above grade (Bedroom.AbvGr). Pick a model selection or model averaging method covered in the Specialization, and describe how this method works. Then, use this method to find the best multiple regression model for predicting the natural log of the home prices.


```{r Q4}
# We set up a new dataset with explanatory variables and respondent variable "price"
new1 <- ames_train %>%
  select(price, Lot.Area, Land.Slope, Year.Built, Year.Remod.Add, Bedroom.AbvGr) 

summary(new1)
```

```{r}
#Set up our initial model
#log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr

initial_model <- lm(log(price) ~., data = new1) 

summary(initial_model)
```

* * *


Now let's use the Backward Elimination method with main fous on adjusted R-squared value.

From the summary of our initial model, we got R-squared value of 0.5625 and Adjusted R-squared at 0.5598. Since p-value here is below 0.05, all our selected explanatory variables are valid for this model. Next, we would check whether removing any of these explanatory variables would give rise to a higher Adjusted R-squared. 


* * *
```{r}
#Remove Lot.Area 
model_noLA <- lm(log(price) ~ Land.Slope + Year.Built + Year.Remod.Add + Bedroom.AbvGr, data=ames_train) 

#Remove Land.Slope
model_noLS <- lm(log(price) ~ Lot.Area +Year.Built + Year.Remod.Add + Bedroom.AbvGr, data = ames_train)

#Remove Year.Built
model_noYB <- lm(log(price) ~ Lot.Area + Land.Slope + Year.Remod.Add + Bedroom.AbvGr, data = ames_train)

#Remove Year.Remod.Add
model_noYRA <- lm(log(price) ~ Lot.Area + Land.Slope + Year.Built + Bedroom.AbvGr, data = ames_train)

#Remove Bedroom.AbvGr
model_noBA <- lm(log(price) ~ Lot.Area + Land.Slope + Year.Built + Year.Remod.Add, data = ames_train)


#Establish new dataset for each Adjusted R-squared for our new selected models

m1 <- summary(model_noLA)$adj.r.squared
m2 <- summary(model_noLS)$adj.r.squared
m3 <- summary(model_noYB)$adj.r.squared
m4 <- summary(model_noYRA)$adj.r.squared
m5 <- summary(model_noBA)$adj.r.squared

R_Squared <- rbind(m1,m2,m3,m4,m5)
removed_models <- c("Lot.Area Removed", "Land.Slope.Removed", "Year.Built Removed", "Year.Remod.Add Removed",
           "Bedroom.AbvGr Removed") 
df <- data.frame(cbind(removed_models, R_Squared)) 

#Add column names for this new data frame
colnames(df) <- c("Models Removed", "Adjusted R_Squared")

df
```

* * * 


Compared with initial Adjusted R-Squared of 0.5598, it's obvious that none of these explanatory variables increased when any of them were removed. Therefore, we should apply all of them for our final model.


* * *

#

Which home has the largest squared residual in the previous analysis (Question 4)? Looking at all the variables in the data set, can you explain why this home stands out from the rest (what factors contribute to the high squared residual and why are those factors relevant)?

```{r}
summary(new1)
```

```{r}
largest_residuals <- which(abs(resid(initial_model))== max(abs(resid(initial_model))))

largest_residuals
```

* * *


The house 428 is with the highest residuals in this dataset. And one apparent feature of this home is its price at $12,789, which is clearly far below the price of other houses given the lot size and the number of bedrooms.


* * *

#

Use the same model selection method you chose in Question 4 to again find the best multiple regression model to predict the natural log of home prices, but this time **replacing Lot.Area with log(Lot.Area)**. Do you arrive at a model including the same set of predictors?


```{r}
#Removed Lot.Area
logmodel_noLA <- lm(log(price) ~ Land.Slope + Year.Built +
                      Year.Remod.Add + Bedroom.AbvGr,
                    data = ames_train)

#Removed Land.Slope
logmodel_noLS <- lm(log(price) ~ log(Lot.Area) + Year.Built + Year.Remod.Add + Bedroom.AbvGr,
                    data = ames_train)

#Removed Year.Built
logmodel_noYB <- lm(log(price) ~ log(Lot.Area) + Land.Slope + Year.Remod.Add + Bedroom.AbvGr,
                    data = ames_train) 

#Removed Year.Remod.Add
logmodel_noYRA <- lm(log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Bedroom.AbvGr,
                     data = ames_train)

#Removed Bedroom.AbvGr
logmodel_noBA <- lm(log(price) ~ log(Lot.Area) + Land.Slope + Year.Built + Year.Remod.Add,
                    data = ames_train)

#Establish new dataset for each Adjusted R-squared for our new selected models
m21 <- summary(logmodel_noLA)$adj.r.squared
m22 <- summary(logmodel_noLS)$adj.r.squared
m23 <- summary(logmodel_noYB)$adj.r.squared
m24 <- summary(logmodel_noYRA)$adj.r.squared
m25 <- summary(logmodel_noBA)$adj.r.squared

R_Squared2 <- rbind(m21, m22, m23, m24, m25)
model2 <- c("Removed Lot.Area", "Removed Land.Slope", "Removed Year.Built", "Removed Remod.Add", 
            "Removed BedroomAbvGr")

df <- data.frame(cbind(model2, R_Squared2)) 

colnames(df) <- c("Model", "Adjusted R_Squared")

df

```

```{r}
#Recall model with log(Lot.Area)
logmodel <- lm(log(price) ~ log(Lot.Area) + Land.Slope + Year.Built +Year.Remod.Add + Bedroom.AbvGr,
         data = ames_train)
summary(logmodel)
```

The model with log(Lot.Area) as one of its explanatory variables is with Adjusted R-squared at 0.6032, while it's clear that none of the adjusted R-squared values increase when any of the explanatory variables are excluded. As a result, we'll stay the same with our model. 

* * *

#

Do you think it is better to log transform Lot.Area, in terms of assumptions for linear regression? Make graphs of the predicted values of log home price versus the true values of log home price for the regression models selected for Lot.Area and log(Lot.Area). Referencing these two plots, provide a written support that includes a quantitative justification for your answer in the first part of question 7.

```{r Q7}
#Set up the dataframe with no log in Lot.Area
ames_no_log <- lm(ames_train$price ~ ames_train$Year.Built + ames_train$Year.Remod.Add +ames_train$Lot.Area + ames_train$Bedroom.AbvGr +ames_train$Land.Slope)

#Set up a scatter plot of predicted log home price vs actual log home price using the model selected
#with the 5 given predictors, including Lot.Area, including a best-fit line
ames_pred <- predict(ames_no_log, ames_train)
plot(ames_pred, ames_train$price)
abline(lm(ames_train$price ~ ames_pred), col = "blue")

```

```{r}
#Set up the dataframe with log in Lot.Area
ames_log <- lm(ames_train$price ~ ames_train$Year.Built + ames_train$Year.Remod.Add + log(ames_train$Lot.Area) + ames_train$Bedroom.AbvGr +ames_train$Land.Slope)

#Set up a scatter plot of predicted log home price vs actual log home price using the model selected
#with the 5 given predictors, including log(Lot.Area), including a best-fit line
ames_pred2 <- predict(ames_log, ames_train)
plot(ames_pred2, ames_train$price)
abline(lm(ames_train$price ~ ames_pred2), col = "red")

```

```{r}
#Check the first plot's R-Squared
summary(lm(ames_train$price ~ ames_pred))$r.squared
```

```{r}
#Check the second plot's R-Squared
summary(lm(ames_train$price ~ ames_pred2))$r.squared
```

* * *


By comparing the two plots, it's clear that the second plot is with more even distribution than the first plot does. Meanwhile, by checking R-squared value of two plots, the first one is 0.4910528, less than the second one with 0.5299. This indicates the log transformation results in a larger R squared, implying that we should use the model with log transforming Lot. Area.


* * *